零基础入门深度学习(6) - 长短时记忆网络(LSTM)
https://zybuluo.com/hanbingtao/note/581764
理解LSTM的翻译
https://www.jianshu.com/p/9dc9f41f0b29
斯坦福 语言模型 RNN LSTM GRU
https://blog.csdn.net/han_xiaoyang/article/details/51932536
RNN BPTT算法
https://zhuanlan.zhihu.com/p/26892413

矩阵求导术
https://zhuanlan.zhihu.com/p/24709748

论文查询点
https://arxiv.org/
http://proceedings.mlr.press/
LSTM的成功应用介绍
http://karpathy.github.io/2015/05/21/rnn-effectiveness/


* 知识回顾

  循环神经网络, 训练方法BPTT
  循环神经网络RNN 很难训练的原因, 导致很难处理 长时依赖.
  
  RNN难训练原因:
  残差沿时间反向传播时, 当前时间t, 之前时间k, 
  残差会变为 W 和 diag(f'(net_h)) 乘积 的 (t-k)次方,
  越远时间的hide状态输出的残差 会变得极小 或者极大. 梯度消失, 或者梯度爆炸
  只有让 W * diag(f'(net_h)) 近似为1 才能避免 梯度消失,梯度爆炸问题.
  但是还是难以抵挡指数函数的威力．10 20 的序列已经是 RNN的极限.
  
* 引入LSTM
  对 RNN 较远时间的网络状态的残差极小,参数梯度极小, 意味着较远记忆对当前的贡献极小.
  被参数遗忘.(t-3 的梯度就已经很小了)
  RNN的长时记忆 因为参数梯度极小的问题 给遗忘了, 只对于短期记忆h 非常敏感.

  LSTM 由 Hochreiter和Schmidhuber 发明, 为了解决 长时记忆被遗忘的问题,
  增加了一个新的记忆输出c, 为了保存长时记忆.
  
  c 叫做 单元状态
  h      状态

  LSTM 或者 RNN的主要问题就是怎样保持长时记忆 c.
  LSTM 通过门控, 选择性的保存长时记忆c. 数据到

* Main

  LSTM的关键，就是怎样控制长期状态c。
  在这里，LSTM的思路是使用三个控制开关。

  第一个开关，负责控制继续保存长期状态c； 
  遗忘门 根据当前输入x 以及短时记忆h 生成门 控制保存长时记忆c.

  第二个开关，负责控制把即时状态c'输入到长期状态c；
  输入门 根据当前输入x 以及短时记忆h 生成门 将当前抽象数据o tanh(x + h_t-1) 加入长时记忆c

  第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出。
  输出门 根据当前输入x 以及短时记忆h 生成门 从长时记忆c 中
  抽取当前时间t的重要信息(因为已经通过输入门将当前数据加入长时记忆c ) 输出为短时记忆h




  新记忆产生：这个状态和GRUs中的新记忆产生状态是一样的。
  我们必须使用输入词xt和过去隐层状态ht−1来产生新的记忆c̃ t，包括了新词x(t)

  输入门：在产生新记忆之前，我们需要判定一下我们当前看到的新词到底重不重要，这就是输入门的作用。
  输入门根据输入词和过去隐层状态共同判定输入值是否值得保留，
  从而判定它以何种程度参与生成新的记忆(或者说对新的记忆做一个约束)。
  因此，它可以作为输入信息更新的一个指标。
  
  遗忘门：这个门和输入门很类似。但是它不能决定输入词有效，它能对过去记忆单元是否对当前记忆单元的计算有用做出评估。

  最终记忆产生：这个阶段会根据遗忘门的作用结果，合理地忘记部分过去的记忆ct−1。
  再根据输入门it的作用结果，产生新记忆c̃t。它将这两个结果加融合起来产生了最终的记忆ct。
  
  输出门：这是一个GRUs里没有显性存在的门。它的目的是从隐层状态分离最终的记忆。
  最终记忆ct包含了大量不必需要保存在隐层状态的信息，这个门限能够评估关于记忆ct哪部分需要显示在隐层状态ht中。
  用于评估这部分信息的中间信号叫做ot，它和tanh(ct)的点乘组成最后的ht。





  
  

  
  
  
  
  
  
  
  
